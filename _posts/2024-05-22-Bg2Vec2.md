## Bg2Vec: Preparing the Training Data 

In this post, we will prepare the training data for the Bg2Vec model. We will use the [Bulgarian Wikipedia](https://bg.wikipedia.org/) as our training data.

First, we will download the latest dump of the Bulgarian Wikipedia and push it to huggingface.co. Check out the result [here](https://huggingface.co/datasets/mboyanov/bgwiki).


Afterwards, we will preprocess the dataset so it is suitable for training the model with llm2vec and bggpt.

## Preparing the Data

To get our hands on the data, we can go to [dumps.wikimedia.org](https://dumps.wikimedia.org) and download the [latest dump](https://dumps.wikimedia.org/bgwiki/20240501/) of the Bulgarian Wikipedia. 
The dump is in XML format and contains all the articles in the Wikipedia.

We will use the `wikiextractor` tool to extract the text from the XML dump. The tool can be found [here]()

Then we can use it as follows:

```
wikiextractor bgwiki-20240420-pages-meta-current.xml  --json -b 5000M
```

This will extract the text from the XML dump and save it in JSON format in a directory called `text`. 
The text will be split into files of 5000MB each which results in a single file in our case. 

Inspecting the data, we can see that we have quite a bunch of empty articles (which simply redirect to other articles).
We will remove these articles as they do not contain any useful information.

Finally, we will save the resulting dataset in .parquet file so that it works better with the huggingface datasets library.

For reusability I have already prepared the dataset and uploaded it to the huggingface datasets library. You can find it [here](https://huggingface.co/datasets/mboyanov/bgwiki).

## Preprocessing

