## Bg2Vec: Preparing the Training Data 

In this post, we will prepare the training data for the Bg2Vec model. We will use the [Bulgarian Wikipedia](https://bg.wikipedia.org/) as our training data.

First, we will download the latest dump of the Bulgarian Wikipedia and push it to huggingface.co. Check out the result [here](https://huggingface.co/datasets/mboyanov/bgwiki).


Afterwards, we will preprocess the dataset so it is suitable for training the model with llm2vec and bggpt.

## Preparing the Data

To get our hands on the data, we can go to [dumps.wikimedia.org](https://dumps.wikimedia.org) and download the [latest dump](https://dumps.wikimedia.org/bgwiki/20240501/) of the Bulgarian Wikipedia. 
The dump is in XML format and contains all the articles in the Wikipedia.

We will use the `wikiextractor` tool to extract the text from the XML dump. The tool can be found [here]()

Then we can use it as follows:

```
wikiextractor bgwiki-20240420-pages-meta-current.xml  --json -b 5000M
```

This will extract the text from the XML dump and save it in JSON format in a directory called `text`. 
The text will be split into files of 5000MB each which results in a single file in our case. 

Inspecting the data, we can see that we have quite a bunch of empty articles (which simply redirect to other articles).
We will remove these articles as they do not contain any useful information.

Finally, we will save the resulting dataset in .parquet file so that it works better with the huggingface datasets library.

For reusability I have already prepared the dataset and uploaded it to the huggingface datasets library. You can find it [here](https://huggingface.co/datasets/mboyanov/bgwiki).

And to use it in your code, you can simply issue:

```python
from datasets import load_dataset

dataset = load_dataset("mboyanov/bgwiki")
```


## Preprocessing

Preprocessing the data consists in the following steps:

1. Tokenization - tokenization is the process of splitting the text into tokens. Tokenization is model-specific, so we will use the tokenizer from the BgGPT model.
2. Chunking - we will group the articles into chunks of 512 tokens. This is an optimization step related to optimal usage of the GPU memory.

Let's start with the tokenization by loading the BgGPT tokenizer.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("INSAIT-Institute/BgGPT-7B-Instruct-v0.2")
```

Let's explore how we can use the tokenizer. The tokenizer has a `tokenize` method which splits the text into tokens 
and a `__call__` method which returns the input_ids and attention_mask of the tokenized text. 
Note how we can also specify `return_tensors='pt'`, so the return type is pytorch tensors.
There is also a `decode` method which converts the input_ids back to text.

```python
sent = "Здравейте, как сте?"
tokenizer.tokenize(sent)
> ['▁Здраве', 'йте', ',', '▁как', '▁сте', '?']

tokenizer(sent)
> {'input_ids': [1, 35834, 32111, 28725, 8600, 19776, 28804], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}

tokenizer(sent, return_tensors='pt')
> {'input_ids': tensor([[    1, 35834, 32111, 28725,  8600, 19776, 28804]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}

tokenizer.decode([1, 35834, 32111, 28725, 8600, 19776, 28804])
'<s> Здравейте, как сте?'
```

Ok, so now we need to apply the tokenization to the dataset. We will use the `map` method of the dataset object to apply the tokenization to all articles.

```python
def tokenize_dataset(raw_datasets, tokenizer, data_args, training_args, text_column_name="text"):
    column_names = list(raw_datasets["train"].features)
    assert text_column_name in column_names, f"Provided text_column_name {text_column_name} not in dataset"
    def tokenize_function(examples):
        return tokenizer(
            examples[text_column_name], return_special_tokens_mask=True
        )

    with training_args.main_process_first(desc="dataset map tokenization"):
        tokenized_datasets = raw_datasets.map(
            tokenize_function,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            remove_columns=column_names,
            load_from_cache_file=not data_args.overwrite_cache,
            desc="Running tokenizer on every text in dataset",
        )
        
    return tokenized_datasets
```


